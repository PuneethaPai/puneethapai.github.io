---
title: "2020-08-12"
date: 2020-08-11T13:52:43+05:30
---

##### GAN for abstractive summarisation:

- Finished reading paper: Higlights are uploaded [here](https://github.com/PuneethaPai/puneethapai.github.io/blob/source/static/papers/Generative%20Adversarial%20Network%20for%20Abstractive%20Text%20Summarization.pdf)

##### Get To The Point - Summarization with Pointer-Generator Networks:

- Finished reading paper: Highlights are uploaded [here](https://github.com/PuneethaPai/puneethapai.github.io/blob/source/static/papers/Get%20To%20The%20Point%20-%20Summarization%20with%20Pointer-Generator%20Networks.pdf)
- Understood the pointer generator mechanism to be soft switch to select word token b/w source text attention and Pvoc.
- This gives the model edge as it can select OOV(Out of Vocabulary) word and also from vocabulary
- Coverage Mechanism: coverage vector ct, which is the sum of attention distributions over all previous decoder timesteps:
  - covlosst =âˆ‘imin(ati,cti)

##### General defination of Batch, Epoch, Iteration:

- Epoch: Number of time you go over the data set
- Batch: Consists of sample of you data for which you compute gradient and update weights:
  - Training on single batch involve forward pass + Backward pass
  - Forward Pass: Compute Loss
  - Backward Pass: Compute Gradient and Update weights
- Iteration: Completion of 1 batch == 1 iteration
  - Thus in 1 Epoch there will be |Trainin Data Size| / |Batch Sizee| number of iterations

Example: If I have training data (100000 samples) and I consider batch_size = 100, then train for 50 Epochs means I will have

Number of Iterations = 50 \* (100000 / 100) = 50000 iterations.
