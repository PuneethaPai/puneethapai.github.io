---
title: "September 1 2020"
date: 2020-09-01T11:44:52+05:30
categories: ["course", "logs"]
tags: ["fastbook", "fastAI", "p-value"]
---

#### Catostrophic forgetting:

- Fine tuned model for a specific task would be less good at the previous
  generic task on which it was pre-trained.
- This is called catastrophic forgetting.
- To retain the earlier learning you would want to revision by feeding few
  samples from previous task along with samples from our new specific task.

#### Are p-values really that valuables?

- In hypothesis testing _(I do get confused everytime)_ based on value of p
  we may either reject the null hypothesis (Ho) else we fail to reject Ho.
  Ho is never truly accepted.
- Inference itself is hard, statistical inference is much harder. All you
  can do is remove all other possibilities and finally conclude this is right
  inference.
- Here is a statement from American Statistical Association. [Statement](https://github.com/PuneethaPai/puneethapai.github.io/blob/source/static/papers/p-valuestatement.pdf)
