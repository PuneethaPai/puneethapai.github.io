---
title: "Aug 28, 2020"
date: 2020-08-28T19:26:03+05:30
draft: true
---

### FastAI Deep learning for coders:

Course Link: https://course.fast.ai/videos/?lesson1=

Lesson 1:

{{< youtube _QUEXsHfsA0>}}

#### Data and Model Ethics:

- We have to take care how model interacts with it's environment
- Model is just a proxy of actual underlying process and as biased
  as the data used for teraining
- Model recieving feedback and learning in production can sometime
  create a positive loop to increase inherant bias in the system
- Example: Black people in US getting arrested 7 times more for drug
  trafficing. This can create positive feedback loop and increase the
  bias to even larger extent.

#### Parallel Distributed Processing (PDP):

1. A set of processing units
1. A state of activation
1. An output function for each unit
1. A pattern of connectivity among units
1. A propagation rule for propagating patterns of activities through
   the network of connectivities
1. An activation rule for combining the inputs impinging on a unit with
   the current state of that unit to produce an output for the unit
1. A learning rule whereby patterns of connectivity are modified by experience
1. An environment within which the system must operate

#### Metrics vs Loss:

- Metrics: Are something which we care about. Accuracy, error_rate, AUC_ROC, F-Beta,
  ROUGE-N, BLEU, METOR, etc
- Loss: Is what comuputer uses as proxy for matrix to learn the task in hand.
  This is because most often **metrics are not smooth function w.r.t to parameters**.
  So changing parameters slightly won't effect metrics and will be hard for the model
  to learn. Where as loss function should always change w.r.t parameter.
- IMP: while checking for overfitting we have to compare validation metric
  and not validation loss. Validaiton loss can still worse as you train for more
  epochs on train set, but validation metrics can still improve.

#### Facts:

- As you increase #of epochs train loss always decreases. This is because
  we do gradient update to lower the loss function.
- Transfer learning, i.e finetune of pretrained model for few epochs in our data gives
  far better model than model trained from scratch for same number of epochs.
