<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>data ethics on Puneeth</title><link>https://puneethapai.github.io/categories/data-ethics/</link><description>Recent content in data ethics on Puneeth</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 04 Sep 2020 19:55:41 +0530</lastBuildDate><atom:link href="https://puneethapai.github.io/categories/data-ethics/index.xml" rel="self" type="application/rss+xml"/><item><title>September 4, 2020</title><link>https://puneethapai.github.io/logs/2020/september/4/</link><pubDate>Fri, 04 Sep 2020 19:55:41 +0530</pubDate><guid>https://puneethapai.github.io/logs/2020/september/4/</guid><description>Data Ethics: Feedback, automation is nice thing, but we need to have proper monitoring reporting stuff in place to check for bias in model prediction. Especially when the model decides it&amp;rsquo;s next set of input we need to be careful to verify that it is learning on general representative data. All data is biased, but trasperancy about how, why it was collected, etc can help us understand inherant bias in our data set.</description></item></channel></rss>