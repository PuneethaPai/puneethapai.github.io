<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Puneeth</title><link>https://puneethapai.github.io/</link><description>Recent content on Puneeth</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 01 Sep 2020 11:44:52 +0530</lastBuildDate><atom:link href="https://puneethapai.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>September 1 2020</title><link>https://puneethapai.github.io/logs/2020/september/1/</link><pubDate>Tue, 01 Sep 2020 11:44:52 +0530</pubDate><guid>https://puneethapai.github.io/logs/2020/september/1/</guid><description>Catostrophic forgetting: Fine tuned model for a specific task would be less good at the previous generic task on which it was pre-trained. This is called catastrophic forgetting. To retain the earlier learning you would want to revision by feeding few samples from previous task along with samples from our new specific task. Are p-values really that valuables? In hypothesis testing (I do get confused everytime) based on value of p we may either reject the null hypothesis (Ho) else we fail to reject Ho.</description></item><item><title>Aug 27, 2020</title><link>https://puneethapai.github.io/logs/2020/august/27/</link><pubDate>Thu, 27 Aug 2020 15:18:11 +0530</pubDate><guid>https://puneethapai.github.io/logs/2020/august/27/</guid><description>FastAI Deep learning for coders: Course Link: https://course.fast.ai/videos/?lesson1=
Note: I had started with Machine Learning course from FastAI Link: https://course18.fast.ai/ml.html
Lesson 1:
Summary: Myths and actual truth around practical Deeplearning. Top down approach for practical DL over bottom up approach followed in academia. Base ball analogy for the same. Libraries covered Python -&amp;gt; Pytorch -&amp;gt; FastAI. Jupyter Notebooks: REPL (Read Execute Print Look). Similar to shell/terminal.</description></item><item><title>Aug 18, 2020</title><link>https://puneethapai.github.io/logs/2020/august/18/</link><pubDate>Tue, 18 Aug 2020 11:57:52 +0530</pubDate><guid>https://puneethapai.github.io/logs/2020/august/18/</guid><description>Hugo Larochell Neural Networks Back Propagation Algorithm: Validation of Back propagation of result using simple limit epsilon tends to zero
Regularization: Generally applied only to weights not baiases L2 Regularization: Takes square of weights Gaussian Prior, in probabilistic modelling says how the weights are generated L1 Regularization: Takes absolute values Laplacian Prior Makes some weights exactly=0 thus pruning connections Makes NNs less complex and to overfit data Generalization: Bias:</description></item><item><title>Aug 12, 2020</title><link>https://puneethapai.github.io/logs/2020/august/12/</link><pubDate>Tue, 11 Aug 2020 13:52:43 +0530</pubDate><guid>https://puneethapai.github.io/logs/2020/august/12/</guid><description>GAN for abstractive summarisation: Finished reading paper: Higlights are uploaded here Get To The Point - Summarization with Pointer-Generator Networks: Finished reading paper: Highlights are uploaded here Understood the pointer generator mechanism to be soft switch to select word token b/w source text attention and Pvoc. This gives the model edge as it can select OOV(Out of Vocabulary) word and also from vocabulary Coverage Mechanism: coverage vector ct, which is the sum of attention distributions over all previous decoder timesteps: covlosst =∑imin(ati,cti) General defination of Batch, Epoch, Iteration: Epoch: Number of time you go over the data set Batch: Consists of sample of you data for which you compute gradient and update weights: Training on single batch involve forward pass + Backward pass Forward Pass: Compute Loss Backward Pass: Compute Gradient and Update weights Iteration: Completion of 1 batch == 1 iteration Thus in 1 Epoch there will be |Trainin Data Size| / |Batch Sizee| number of iterations Example: If I have training data (100000 samples) and I consider batch_size = 100, then train for 50 Epochs means I will have</description></item><item><title>Aug 08, 2020</title><link>https://puneethapai.github.io/logs/2020/august/8/</link><pubDate>Sat, 08 Aug 2020 10:43:41 +0530</pubDate><guid>https://puneethapai.github.io/logs/2020/august/8/</guid><description>Exploring Abstractive vs Extractive Text summarization started with huggingface/transormers default pipeline model Really well trained but thought it was only extractive. We needed abstractive summary, also way to train model with data = (man page content, tldr summary) Found way to parse man pages for commands using groff $ groff -man -T utf8 $(man -w cp) $ tldr groff groff Typesetting program that reads plain text mixed with formatting commands and produces formatted output.</description></item><item><title>Aug 05, 2020</title><link>https://puneethapai.github.io/logs/2020/august/5/</link><pubDate>Wed, 05 Aug 2020 11:07:49 +0530</pubDate><guid>https://puneethapai.github.io/logs/2020/august/5/</guid><description>Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 2 - Multi-Task &amp;amp; Meta-Learning Basics
Notes:
while training a Classification NN, we will have learnt theta min for L(theta, f, D) is minimum we have final layer where P(y/x) is predicted If we compute P(a/y) at previous layers and reduce n/w computations How much back we can come in layers? Will it actually reduce computation and optimises state of the art NN?</description></item><item><title>Just a dream. or is it !!</title><link>https://puneethapai.github.io/blogs/thoughts/just-a-dream-or-is-it/</link><pubDate>Mon, 29 Jun 2020 22:22:14 +0530</pubDate><guid>https://puneethapai.github.io/blogs/thoughts/just-a-dream-or-is-it/</guid><description>Suddenly I was following a woman carrying a huge box in her hands. I wasn’t sure why we both were in a hurry. I followed her to her place.
There she was her sister whom I thought was cute. She looked at me, we were sharing a surprised look. I could notice she was a little frustrated over something her elder sister had done.
They both started to argue on something.</description></item><item><title>Hugo</title><link>https://puneethapai.github.io/blogs/miscellaneous/hugo/</link><pubDate>Wed, 08 Apr 2020 20:31:23 +0530</pubDate><guid>https://puneethapai.github.io/blogs/miscellaneous/hugo/</guid><description>Get Started Create new site:
hugo new site &amp;lt;name&amp;gt; cd &amp;lt;name&amp;gt; Download required them into themes folder: You can choose to create git submodule or clone the theme
git clone/sub module http://theme-from-github into themes/&amp;lt;name&amp;gt; Update config.yaml to choose the downloaded theme.
theme:&amp;lt;name&amp;gt; Start hugo server:
hugo server Create Content All your site content resides in folder content/
hugo new &amp;lt;name&amp;gt;.md Each content file created will have metadata associated known as frontmatter.</description></item><item><title>About</title><link>https://puneethapai.github.io/about/</link><pubDate>Sat, 04 Apr 2020 10:09:14 +0530</pubDate><guid>https://puneethapai.github.io/about/</guid><description>Github Linked IN Resume</description></item></channel></rss>