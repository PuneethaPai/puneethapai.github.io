<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Puneeth</title><link>https://puneethapai.github.io/</link><description>Recent content on Puneeth</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 19 Sep 2020 20:50:42 +0530</lastBuildDate><atom:link href="https://puneethapai.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>September 19, 2020</title><link>https://puneethapai.github.io/logs/2020/september/19/</link><pubDate>Sat, 19 Sep 2020 20:50:42 +0530</pubDate><guid>https://puneethapai.github.io/logs/2020/september/19/</guid><description>CSV dict writer: Dict Reader and Writer are most flexible ways to manipulate csv data.
Both reader and writer supports generators and hence can work in low memory systems pretty efficiently.
Example: import csv from typing import Iterable, Dict def read_csv(path: str) -&amp;gt; Iterable[Dict]: &amp;#34;&amp;#34;&amp;#34;Reads csv and yeilds row as dict&amp;#34;&amp;#34;&amp;#34; with open(path, &amp;#34;r&amp;#34;, newline=&amp;#34;&amp;#34;) as f: reader = csv.DictReader(f) for row in reader: yield row def write_csv(path: str, data: Iterable[Dict], header: list = None) -&amp;gt; None: &amp;#34;&amp;#34;&amp;#34;Writes data: Iterable[Dict] into csv file&amp;#34;&amp;#34;&amp;#34; if not header: header = data[0].</description></item><item><title>September 18, 2020</title><link>https://puneethapai.github.io/logs/2020/september/18/</link><pubDate>Fri, 18 Sep 2020 19:44:47 +0530</pubDate><guid>https://puneethapai.github.io/logs/2020/september/18/</guid><description>Python Debugging pdb is a standard python debuger and to invoke it we can call brakpoint() in scripts or use %pdb magic in iPython. Documentation
In pdb pythons standard debugger, help command directs you further. Some shortcuts are n next line, s step into, c for continue. You can do anything w.r.t ipython shell.
IPython trics: All and more about useful ipython %magic and debugging can be found here:
Great reference: https://jakevdp.</description></item><item><title>The Only Way Is Through: Part 2</title><link>https://puneethapai.github.io/blogs/miscellaneous/only-way-is-through-part-2/</link><pubDate>Thu, 17 Sep 2020 16:58:43 +0530</pubDate><guid>https://puneethapai.github.io/blogs/miscellaneous/only-way-is-through-part-2/</guid><description>Do read Part 1 if you haven't yet. Thanks!
I do believe that the only way is through. I agree that in this fast paced society, we may not get enough time to explore the concepts and learn to the depth we would want to. It is easier to fall back to &amp;ldquo;finding shortcuts, quick answers&amp;rdquo; strategy.
Even though I say all these, I truly am greatful to the larger community and platforms like Stackoverflow, Quora, chat forums, etc.</description></item><item><title>The Only Way Is Through: Part1</title><link>https://puneethapai.github.io/blogs/miscellaneous/only-way-is-through/</link><pubDate>Wed, 16 Sep 2020 07:00:17 +0530</pubDate><guid>https://puneethapai.github.io/blogs/miscellaneous/only-way-is-through/</guid><description>It has been 4 years in my professional career. Till date I have been always finding cool shortcuts to achieve the task or learn some new thing for career growth, etc.
It has served me well, so far at least. But lately it is becoming more evident, that the idealogy to keep finding shortcuts, solved examples is not working out. I am missing the actual point. Instead of learning or doing the task, in right way, I keep spending my time, energy and focus into finding the best solution awailable, easy ways to do, quick starts, ready made solution etc.</description></item><item><title>September 4, 2020</title><link>https://puneethapai.github.io/logs/2020/september/4/</link><pubDate>Fri, 04 Sep 2020 19:55:41 +0530</pubDate><guid>https://puneethapai.github.io/logs/2020/september/4/</guid><description>Data Ethics: Feedback, automation is nice thing, but we need to have proper monitoring reporting stuff in place to check for bias in model prediction. Especially when the model decides it's next set of input we need to be careful to verify that it is learning on general representative data. All data is biased, but trasperancy about how, why it was collected, etc can help us understand inherant bias in our data set.</description></item><item><title>September 1, 2020</title><link>https://puneethapai.github.io/logs/2020/september/1/</link><pubDate>Tue, 01 Sep 2020 11:44:52 +0530</pubDate><guid>https://puneethapai.github.io/logs/2020/september/1/</guid><description>Catostrophic forgetting: Fine tuned model for a specific task would be less good at the previous generic task on which it was pre-trained. This is called catastrophic forgetting. To retain the earlier learning you would want to revision by feeding few samples from previous task along with samples from our new specific task. Are p-values really that valuables? In hypothesis testing (I do get confused everytime) based on value of p we may either reject the null hypothesis (Ho) else we fail to reject Ho.</description></item><item><title>Aug 27, 2020</title><link>https://puneethapai.github.io/logs/2020/august/27/</link><pubDate>Thu, 27 Aug 2020 15:18:11 +0530</pubDate><guid>https://puneethapai.github.io/logs/2020/august/27/</guid><description>FastAI Deep learning for coders: Course Link: https://course.fast.ai/videos/?lesson1=
Note: I had started with Machine Learning course from FastAI Link: https://course18.fast.ai/ml.html
Lesson 1:
Summary: Myths and actual truth around practical Deeplearning. Top down approach for practical DL over bottom up approach followed in academia. Base ball analogy for the same. Libraries covered Python -&amp;gt; Pytorch -&amp;gt; FastAI. Jupyter Notebooks: REPL (Read Execute Print Look). Similar to shell/terminal.</description></item><item><title>Aug 18, 2020</title><link>https://puneethapai.github.io/logs/2020/august/18/</link><pubDate>Tue, 18 Aug 2020 11:57:52 +0530</pubDate><guid>https://puneethapai.github.io/logs/2020/august/18/</guid><description>Hugo Larochell Neural Networks Back Propagation Algorithm: Validation of Back propagation of result using simple limit epsilon tends to zero
Regularization: Generally applied only to weights not baiases L2 Regularization: Takes square of weights Gaussian Prior, in probabilistic modelling says how the weights are generated L1 Regularization: Takes absolute values Laplacian Prior Makes some weights exactly=0 thus pruning connections Makes NNs less complex and to overfit data Generalization: Bias:</description></item><item><title>Aug 12, 2020</title><link>https://puneethapai.github.io/logs/2020/august/12/</link><pubDate>Tue, 11 Aug 2020 13:52:43 +0530</pubDate><guid>https://puneethapai.github.io/logs/2020/august/12/</guid><description>GAN for abstractive summarisation: Finished reading paper: Higlights are uploaded here Get To The Point - Summarization with Pointer-Generator Networks: Finished reading paper: Highlights are uploaded here Understood the pointer generator mechanism to be soft switch to select word token b/w source text attention and Pvoc. This gives the model edge as it can select OOV(Out of Vocabulary) word and also from vocabulary Coverage Mechanism: coverage vector ct, which is the sum of attention distributions over all previous decoder timesteps: covlosst =∑imin(ati,cti) General defination of Batch, Epoch, Iteration: Epoch: Number of time you go over the data set Batch: Consists of sample of you data for which you compute gradient and update weights: Training on single batch involve forward pass + Backward pass Forward Pass: Compute Loss Backward Pass: Compute Gradient and Update weights Iteration: Completion of 1 batch == 1 iteration Thus in 1 Epoch there will be |Trainin Data Size| / |Batch Sizee| number of iterations Example: If I have training data (100000 samples) and I consider batch_size = 100, then train for 50 Epochs means I will have</description></item><item><title>Aug 08, 2020</title><link>https://puneethapai.github.io/logs/2020/august/8/</link><pubDate>Sat, 08 Aug 2020 10:43:41 +0530</pubDate><guid>https://puneethapai.github.io/logs/2020/august/8/</guid><description>Exploring Abstractive vs Extractive Text summarization started with huggingface/transormers default pipeline model Really well trained but thought it was only extractive. We needed abstractive summary, also way to train model with data = (man page content, tldr summary) Found way to parse man pages for commands using groff $ groff -man -T utf8 $(man -w cp) $ tldr groff groff Typesetting program that reads plain text mixed with formatting commands and produces formatted output.</description></item><item><title>Aug 05, 2020</title><link>https://puneethapai.github.io/logs/2020/august/5/</link><pubDate>Wed, 05 Aug 2020 11:07:49 +0530</pubDate><guid>https://puneethapai.github.io/logs/2020/august/5/</guid><description>Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 2 - Multi-Task &amp;amp; Meta-Learning Basics
Notes:
while training a Classification NN, we will have learnt theta min for L(theta, f, D) is minimum we have final layer where P(y/x) is predicted If we compute P(a/y) at previous layers and reduce n/w computations How much back we can come in layers? Will it actually reduce computation and optimises state of the art NN?</description></item><item><title>Just a dream. or is it !!</title><link>https://puneethapai.github.io/blogs/thoughts/just-a-dream-or-is-it/</link><pubDate>Mon, 29 Jun 2020 22:22:14 +0530</pubDate><guid>https://puneethapai.github.io/blogs/thoughts/just-a-dream-or-is-it/</guid><description>Suddenly I was following a woman carrying a huge box in her hands. I wasn’t sure why we both were in a hurry. I followed her to her place.
There she was her sister whom I thought was cute. She looked at me, we were sharing a surprised look. I could notice she was a little frustrated over something her elder sister had done.
They both started to argue on something.</description></item><item><title>Hugo</title><link>https://puneethapai.github.io/blogs/miscellaneous/hugo/</link><pubDate>Wed, 08 Apr 2020 20:31:23 +0530</pubDate><guid>https://puneethapai.github.io/blogs/miscellaneous/hugo/</guid><description>Get Started Create new site:
hugo new site &amp;lt;name&amp;gt; cd &amp;lt;name&amp;gt; Download required them into themes folder: You can choose to create git submodule or clone the theme
git clone/sub module http://theme-from-github into themes/&amp;lt;name&amp;gt; Update config.yaml to choose the downloaded theme.
theme:&amp;lt;name&amp;gt; Start hugo server:
hugo server Create Content All your site content resides in folder content/
hugo new &amp;lt;name&amp;gt;.md Each content file created will have metadata associated known as frontmatter.</description></item><item><title>About</title><link>https://puneethapai.github.io/about/</link><pubDate>Sat, 04 Apr 2020 10:09:14 +0530</pubDate><guid>https://puneethapai.github.io/about/</guid><description>Github Linked IN Resume</description></item></channel></rss>