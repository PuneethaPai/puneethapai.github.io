<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Logs on Puneeth</title><link>https://puneethapai.github.io/logs/</link><description>Recent content in Logs on Puneeth</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 08 Aug 2020 10:43:41 +0530</lastBuildDate><atom:link href="https://puneethapai.github.io/logs/index.xml" rel="self" type="application/rss+xml"/><item><title>2020-08-08</title><link>https://puneethapai.github.io/logs/august-2020/8/</link><pubDate>Sat, 08 Aug 2020 10:43:41 +0530</pubDate><guid>https://puneethapai.github.io/logs/august-2020/8/</guid><description>Exploring Abstractive vs Extractive Text summarization started with huggingface/transormers default pipeline model Really well trained but thought it was only extractive. We needed abstractive summary, also way to train model with data = (man page content, tldr summary) Found way to parse man pages for commands using groff $ groff -man -T utf8 $(man -w cp) $ tldr groff groff Typesetting program that reads plain text mixed with formatting commands and produces formatted output.</description></item><item><title>2020-08-05</title><link>https://puneethapai.github.io/logs/august-2020/5/</link><pubDate>Wed, 05 Aug 2020 11:07:49 +0530</pubDate><guid>https://puneethapai.github.io/logs/august-2020/5/</guid><description>Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 2 - Multi-Task &amp;amp; Meta-Learning Basics
Notes:
while training a Classification NN, we will have learnt theta min for L(theta, f, D) is minimum we have final layer where P(y/x) is predicted If we compute P(a/y) at previous layers and reduce n/w computations How much back we can come in layers? Will it actually reduce computation and optimises state of the art NN?</description></item></channel></rss>