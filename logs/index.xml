<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Logs on Puneeth</title><link>https://puneethapai.github.io/logs/</link><description>Recent content in Logs on Puneeth</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 11 Aug 2020 13:52:43 +0530</lastBuildDate><atom:link href="https://puneethapai.github.io/logs/index.xml" rel="self" type="application/rss+xml"/><item><title>2020-08-12</title><link>https://puneethapai.github.io/logs/august-2020/12/</link><pubDate>Tue, 11 Aug 2020 13:52:43 +0530</pubDate><guid>https://puneethapai.github.io/logs/august-2020/12/</guid><description>GAN for abstractive summarisation: Finished reading paper: Higlights are uploaded here Get To The Point - Summarization with Pointer-Generator Networks: Finished reading paper: Highlights are uploaded here Understood the pointer generator mechanism to be soft switch to select word token b/w source text attention and Pvoc. This gives the model edge as it can select OOV(Out of Vocabulary) word and also from vocabulary Coverage Mechanism: coverage vector ct, which is the sum of attention distributions over all previous decoder timesteps: covlosst =âˆ‘imin(ati,cti) General defination of Batch, Epoch, Iteration: Epoch: Number of time you go over the data set Batch: Consists of sample of you data for which you compute gradient and update weights: Training on single batch involve forward pass + Backward pass Forward Pass: Compute Loss Backward Pass: Compute Gradient and Update weights Iteration: Completion of 1 batch == 1 iteration Thus in 1 Epoch there will be |Trainin Data Size| / |Batch Sizee| number of iterations Example: If I have training data (100000 samples) and I consider batch_size = 100, then train for 50 Epochs means I will have</description></item><item><title>2020-08-08</title><link>https://puneethapai.github.io/logs/august-2020/8/</link><pubDate>Sat, 08 Aug 2020 10:43:41 +0530</pubDate><guid>https://puneethapai.github.io/logs/august-2020/8/</guid><description>Exploring Abstractive vs Extractive Text summarization started with huggingface/transormers default pipeline model Really well trained but thought it was only extractive. We needed abstractive summary, also way to train model with data = (man page content, tldr summary) Found way to parse man pages for commands using groff $ groff -man -T utf8 $(man -w cp) $ tldr groff groff Typesetting program that reads plain text mixed with formatting commands and produces formatted output.</description></item><item><title>2020-08-05</title><link>https://puneethapai.github.io/logs/august-2020/5/</link><pubDate>Wed, 05 Aug 2020 11:07:49 +0530</pubDate><guid>https://puneethapai.github.io/logs/august-2020/5/</guid><description>Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 2 - Multi-Task &amp;amp; Meta-Learning Basics
Notes:
while training a Classification NN, we will have learnt theta min for L(theta, f, D) is minimum we have final layer where P(y/x) is predicted If we compute P(a/y) at previous layers and reduce n/w computations How much back we can come in layers? Will it actually reduce computation and optimises state of the art NN?</description></item></channel></rss>